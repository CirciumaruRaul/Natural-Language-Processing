{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t37YGxm_1R3Q"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD3nf0Xtpwyw"
      },
      "source": [
        "## Morphological analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7JiApI2pyXg"
      },
      "source": [
        "Morphological analysis involves studying the structure and formation of words. Some techniques used in morphological analysis are stemming, lemmatization and POS-tagging. **POS-tagging** is process of assigning part-of-speech tags (noun, verb, adjective etc.) to tokens (words). This is done automatically by using a\n",
        "POS-tagger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-FoAbOVsv_i"
      },
      "source": [
        "### StanfordPOSTagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0xzixUvtO-c"
      },
      "source": [
        "A POS-tagger is a program that tags words in raw text, indicating their part-of-speech. [StanfordPOSTagger](https://nlp.stanford.edu/software/tagger.html) is widely used in NLP tasks. Please download the full version, then unzip the archive and search for *stanford-postagger.jar*. To import the tagger use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yXE6H6yu2kN"
      },
      "outputs": [],
      "source": [
        "from nltk.tag.stanford import StanfordPOSTagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKrAG-7zvFBA"
      },
      "source": [
        "Then create a tagger instance. We will use *english-bidirectional-distsim.tagger*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkkSgT0HvVUE"
      },
      "outputs": [],
      "source": [
        "java_path = \"C:\\\\Program Files\\\\Java\\\\jdk-19\\\\bin\\\\java.exe\"\n",
        "os.environ['JAVAHOME'] = java_path\n",
        "\n",
        "path_to_model = \"stanford-postagger-full-2020-11-17/models/english-bidirectional-distsim.tagger\"\n",
        "path_to_jar = \"stanford-postagger-full-2020-11-17/stanford-postagger.jar\"\n",
        "tagger = StanfordPOSTagger(path_to_model, path_to_jar)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBIvNgEqvrq4"
      },
      "source": [
        "To compute POS tags we use the *tag* method. Note that before we POS-tag a text, we must tokenize it first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKeRW0JSvoz-",
        "outputId": "7b02d84a-4b68-471f-8854-eb913c56f9df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('saw', 'VBD'),\n",
              " ('my', 'PRP$'),\n",
              " ('cat', 'NN'),\n",
              " ('playing', 'VBG'),\n",
              " ('with', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('dog', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tagger.tag([\"I\",\"saw\",\"my\",\"cat\", \"playing\", \"with\", \"a\", \"dog\", \".\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE-__78b1ptC"
      },
      "source": [
        "You can see here [what each tag means](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
        "\n",
        "In case you receive an error while trying to use the tagger, read here: https://stackoverflow.com/questions/34692987/cant-make-stanford-pos-tagger-working-in-nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HfYtXIr2qOc"
      },
      "source": [
        "### NLTK tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "minnah3e2rSb"
      },
      "source": [
        "You can also use NLTK's own POS tagger, although the Stanford POS tagger is often reported to be more accurate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxUNFliy2nJb",
        "outputId": "f8480d2a-c2d4-439b-fa06-69c767b94d7d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARqalat93bhR",
        "outputId": "e7f2c287-e08b-456e-f509-4a4fd82e82a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('saw', 'VBD'),\n",
              " ('my', 'PRP$'),\n",
              " ('cat', 'JJ'),\n",
              " ('playing', 'NN'),\n",
              " ('with', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('dog', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.pos_tag([\"I\",\"saw\",\"my\",\"cat\", \"playing\", \"with\", \"a\", \"dog\", \".\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2N_Z1XI9Gli"
      },
      "source": [
        "You can find the tags meaning with *nltk.help.upenn_tagset(tag)*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In0te6ke9TBo",
        "outputId": "dcdad8d2-4b14-4325-ac5d-b8b4762af79f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets_json to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets_json.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('tagsets_json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCoNoo-A9P6J",
        "outputId": "286c429e-5890-450e-9808-4512185e19ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n"
          ]
        }
      ],
      "source": [
        "nltk.help.upenn_tagset('PRP$')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhyzwqq15U6r"
      },
      "source": [
        "## Syntax analysis (also known as parsing)\n",
        "It is used to obtain the structure of a sentence and the connections between tokens (words) in a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4K3rN6n6_0w"
      },
      "source": [
        "### Parsing with Context-Free Grammars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdDCMafR8yJX"
      },
      "source": [
        "A parser processes input sentences according to the productions of a **grammar**, which is a set of rules used to describe all possible sentences in a language. For example NP -> Det N (a noun phrase can be a determiner followed by a noun). A grammar is \"context-free\" when the rules can be applied without considering surrounding symbols.\n",
        "\n",
        "By convention, the lefthand side of the first production is the start-symbol of the grammar, typically S. All parsing trees must have this symbol as their root label.\n",
        "\n",
        "Note that a production like VP -> V NP | V NP PP is an abbreviation for the two productions VP -> V NP and VP -> V NP PP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XxxQWpL8fjj"
      },
      "source": [
        "#### Recursive Descent Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5LFRvXQ91Fz"
      },
      "source": [
        "This is a top-down parser (it constructs the parse tree top down), that backtracks through the rules, expanding the tree nodes in a depth-first manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0y5qKNWB_xJ",
        "outputId": "5f618761-a701-487c-d150-5df03976ae11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S (NP (PRP I)) (VP (V like) (NP (Det my) (N school))))\n"
          ]
        }
      ],
      "source": [
        "gram = nltk.CFG.fromstring(\"\"\"  S -> NP VP | TO VB\n",
        "VP -> V NP | V NP PP | V PP\n",
        "PP -> P NP\n",
        "V -> \"caught\" | \"ate\" | \"likes\" | \"like\" | \"chase\" | \"go\"\n",
        "NP -> Det N | Det N PP | PRP\n",
        "Det -> \"the\" | \"a\" | \"an\" | \"my\" | \"some\"\n",
        "N -> \"mice\" | \"cat\" | \"dog\" |  \"school\"\n",
        "P -> \"in\" | \"to\" | \"on\"\n",
        "TO -> \"to\"\n",
        "VB -> \"like\" | \"go\" | \"catch\" | \"eat\" | \"chase\"\n",
        "PRP -> \"I\"  \"\"\")\n",
        "\n",
        "# Note that a grammar needs terminal symbols (e.g Det -> \"the\" | \"a\" | \"an\" | \"my\" | \"some\")\n",
        "\n",
        "sent=[\"I\", \"like\", \"my\", \"school\"]\n",
        "rdp = nltk.RecursiveDescentParser(gram)\n",
        "for tree in rdp.parse(sent):\n",
        "    print(tree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwib4q0TDgN0"
      },
      "source": [
        "You can observe the way it works by using the app provided by nltk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdCL9LGtCu8J"
      },
      "outputs": [],
      "source": [
        "nltk.app.rdparser() # does not work in colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqM5CpxKDt0n"
      },
      "source": [
        "#### Shift Reduce Parsing (bottom-up parsing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TASeiwbeHx2y"
      },
      "source": [
        "Note that this parser does not implement any backtracking, so it is not guaranteed to find a parse for a text, even if one exists. Furthermore, it will only find at most one parse, even if more parses exist. On the other hand, Shift-Reduce parsing can deal with productions displaying left recursion (e.g. VP -> VP NP), unlike Recursive Descent parsing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOFCga8hDrqI",
        "outputId": "df7e8786-db18-4cf0-a38c-8cb5b6e33812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: P -> 'to' will never be used\n",
            "(S (NP (PRP I)) (VP (V like) (NP (Det my) (N school))))\n"
          ]
        }
      ],
      "source": [
        "srp = nltk.ShiftReduceParser(gram)\n",
        "sent=[\"I\", \"like\", \"my\", \"school\"]\n",
        "for tree in srp.parse(sent):\n",
        "  print(tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw327v_JEE4o"
      },
      "outputs": [],
      "source": [
        "nltk.app.srparser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syKTUnK9mguV"
      },
      "source": [
        "## Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LWYzQoZmGGZ"
      },
      "source": [
        "1. Choose a wikipedia article. You will download and acces the article using this python module: [wikipedia](https://pypi.org/project/wikipedia/). Use the property *content* to extract the text. Print the title of the chosen article and the first N=200 words from the article to verify that all works well. Print the POS-tagging for the first N=20 sentences. You can use nltk's word_tokenize.\n",
        "\n",
        "2. Create a function that receives a part of speech tag and returns a list with all the words from the text (can be given as a parameter too) that represent that part of speech. Create a function that receives a list of POS tags and returns a list with words having any of the given POS tags (use the first function in implementing the second one).\n",
        "\n",
        "3. Use the function above to print all the nouns (note that there are multiple tags for nouns), and, respectively all the verbs (corresponding to all verb tags). Also, print the percentage of content words (noun+verbs) from the entire text.\n",
        "\n",
        "3. Print a table of four columns. The columns will be separated with the character \"|\". The head of the table will be: **Original word | POS | Simple lemmatization | Lemmatization with POS**. The table will compare the results of lemmatization (WordNetLemmatizer) without giving the part of speech and the lemmatization with the given part of speech for each word. The table must contain only words that give different results for the two lemmatizations (for example, the word \"running\" - without POS, the result will always be \"running\", but with pos=\"v\" it is \"run\"). The table will contain the results for the first N sentences from the text (each row corresponding to a word). Try to print only distinct results inside the table (for example, if a word has two occurnces inside the text, and matches the requirments for appearing in the table, it should have only one corresponding row).\n",
        "\n",
        "5. Print a graphic showing the number of words for each part of speech. If there are too many different parts of speech, you can print only those with a higher number of corresponding words.\n",
        "\n",
        "6. Create your own grammar with different productions and terminal symbols. Apply recursive descent parsing on a sentence with at least 5 different parts of speech and a tree of at least level 4.\n",
        "\n",
        "7. Apply shift reduce parsing on the same sentence and check programatically if the two trees are equal. Find a sentence with equal trees and a sentence with different results (we consider the tree different even when it has no solution for one of the parsers, but has for the other)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Each exercises is done in a separate cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8DUJLXtlIC4"
      },
      "outputs": [],
      "source": [
        "import wikipedia as wiki\n",
        "import wikipedia as wiki\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.tag.stanford import StanfordPOSTagger\n",
        "import os \n",
        "java_path = \"C:\\\\Program Files\\\\Java\\\\jdk-19\\\\bin\\\\java.exe\"\n",
        "os.environ['JAVAHOME'] = java_path\n",
        "\n",
        "path_to_model = \"/Users/raul/Downloads/stanford-postagger-full-2020-11-17/models/english-bidirectional-distsim.tagger\"\n",
        "path_to_jar = \"/Users/raul/Downloads/stanford-postagger-full-2020-11-17/stanford-postagger-4.2.0.jar\"\n",
        "tagger = StanfordPOSTagger(path_to_model, path_to_jar)\n",
        "\n",
        "try:\n",
        "    article = wiki.page(\"bbrain\")\n",
        "    content = article.content\n",
        "    words = word_tokenize(content)  \n",
        "    first_200_words = \" \".join(words[:200])  \n",
        "    print(f\"First 200 words of the article:\\n{first_200_words}\\n\")\n",
        "    sentences = sent_tokenize(content)\n",
        "    first_20_sentences = sentences[:20]\n",
        "    \n",
        "    print(\"Tag the first 20 sentences:\\n\")\n",
        "    for i, sent in enumerate(first_20_sentences, 1):\n",
        "        tokens = word_tokenize(sent)\n",
        "        pos_tag = tagger.tag(tokens)\n",
        "        print(f\"Sentence {i}: {pos_tag}\\n\")\n",
        "\n",
        "except wiki.exceptions.PageError:\n",
        "    print(f\"Error: Wikipedia page bbrain not found.\")\n",
        "except wiki.exceptions.DisambiguationError as e:\n",
        "    print(f\"Disambiguation error: {e.options}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_words_by_pos(text, pos_tag_filter):\n",
        "    words = word_tokenize(text)  \n",
        "    tagged_words = tagger.tag(words)  \n",
        "    return [word for word, tag in tagged_words if tag == pos_tag_filter]\n",
        "\n",
        "def get_words_by_pos_list(text, pos_tags_filter):\n",
        "    words = []\n",
        "    for pos_tag_filter in pos_tags_filter:\n",
        "        words.extend(get_words_by_pos(text, pos_tag_filter))\n",
        "    return words\n",
        "\n",
        "get_words_by_pos(\"The brain is an organ that serves as the center of the nervous system in all vertebrate and most invertebrate animals\", \"NN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "noun_tags = [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] \n",
        "verb_tags = [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
        "\n",
        "\n",
        "try:\n",
        "    article = wiki.page(\"bbrain\")\n",
        "    content = article.content\n",
        "    sentences = sent_tokenize(content) \n",
        "    first_20_sentences = \" \".join(sentences[:20])\n",
        "    nouns = get_words_by_pos_list(first_20_sentences, noun_tags)\n",
        "    verbs = get_words_by_pos_list(first_20_sentences, verb_tags)\n",
        "    content_word_count = len(nouns) + len(verbs)\n",
        "    words = word_tokenize(first_20_sentences)\n",
        "    total_words = len(words) if words else 1\n",
        "    percentage = (content_word_count / total_words) * 100\n",
        "    print(f\"Percentage is: {percentage}\")\n",
        "except wiki.exceptions.PageError:\n",
        "    print(f\"Error: Wikipedia page bbrain not found.\")\n",
        "except wiki.exceptions.DisambiguationError as e:\n",
        "    print(f\"Disambiguation error: {e.options}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wikipedia as wiki\n",
        "import wikipedia as wiki\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.tag.stanford import StanfordPOSTagger\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import os \n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "java_path = \"C:\\\\Program Files\\\\Java\\\\jdk-19\\\\bin\\\\java.exe\"\n",
        "os.environ['JAVAHOME'] = java_path\n",
        "\n",
        "path_to_model = \"/Users/raul/Downloads/stanford-postagger-full-2020-11-17/models/english-bidirectional-distsim.tagger\"\n",
        "path_to_jar = \"/Users/raul/Downloads/stanford-postagger-full-2020-11-17/stanford-postagger-4.2.0.jar\"\n",
        "tagger = StanfordPOSTagger(path_to_model, path_to_jar)\n",
        "\n",
        "print(f\"{'Original Word':<{15}} | {'POS':<{5}} | {'Simple Lemmatization':<{20}} | {'Lemmatization with POS':<{15}}\")\n",
        "print(f\"-\" * 71)\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "try:\n",
        "    article = wiki.page(\"bbrain\")\n",
        "    content = article.content\n",
        "    sentences = sent_tokenize(content) \n",
        "    first_20_sentences = \" \".join(sentences[:20])\n",
        "    words = list(map(str.lower, word_tokenize(first_200_words)))\n",
        "    taggs = tagger.tag(words)\n",
        "    lem = WordNetLemmatizer()\n",
        "    seen = set()\n",
        "    for i, word in enumerate(words, start=0):\n",
        "        if word not in seen:\n",
        "            print(f\"{word:<{15}} | {taggs[i][1]:<{5}} | {lem.lemmatize(word):<{20}} | {lem.lemmatize(word, pos=get_wordnet_pos(taggs[i][1])):<{15}}\")\n",
        "            seen.add(word)\n",
        "\n",
        "except wiki.exceptions.PageError:\n",
        "    print(f\"Error: Wikipedia page bbrain not found.\")\n",
        "except wiki.exceptions.DisambiguationError as e:\n",
        "    print(f\"Disambiguation error: {e.options}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "counter = Counter(second for _, second in taggs)\n",
        "counter = {k: v for k, v in counter.items() if v >= 2}\n",
        "\n",
        "labels, counts = zip(*counter.items())\n",
        "\n",
        "plt.bar(labels, counts, color='red')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exercise 6 - was not done =))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
