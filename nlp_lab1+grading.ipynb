{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnjig6UG3fS7"
      },
      "source": [
        "### Grading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIpDGC4S3uOm"
      },
      "source": [
        "The final grade for the NLP subject is the mean of 2 grades: the exam grade and the laboratory grade.\n",
        "\n",
        "The maximum number of laboratory points is 10. Points are obtained in the following activities:\n",
        "- exercises during class\n",
        "- homework that consists of the exercises not finished during class\n",
        "\n",
        "Exercises finished during class are worth double the points.\n",
        "\n",
        "Until the end of the semester, each student will have to present their homework (the teacher will ask random questions from the submitted exercises)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv_CozD5xzh9"
      },
      "source": [
        "### NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao1ygopaxziA"
      },
      "source": [
        "NLTK (Natural Language Toolkit) is a specialized tool for natural language processing. It is comprised of modules with widely used NLP algorithms as well as tools for using data from many corpora.\n",
        "\n",
        "A corpus is a collection of texts in electronic format, either with raw content (the original text, without adnotations) or with various adnotations (POS, sintactic, semantic)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxQ3G0ZmxziB",
        "outputId": "23e32b0c-5887-4e2b-91ac-82b437f7c757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "#Install NLTK\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm5KdPpaxziC"
      },
      "outputs": [],
      "source": [
        "#Import the nltk module\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40VSCFjhySiA",
        "outputId": "7b781f0d-a161-43fc-97e1-df3c338dc398"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mdSbsjRxziD"
      },
      "source": [
        "### Text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqNOA-92xziH"
      },
      "source": [
        "Tokenization is the process of breaking raw texts into sentences or words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC7kemfTxziI",
        "outputId": "4ae39fc8-0827-42bf-a869-e9c200ade49e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I have two dogs and a cat.', 'Do you have pets too?', 'My cat likes to chase mice.', 'My dogs like to chase my cat.']\n",
            "['I', 'have', 'two', 'dogs', 'and', 'a', 'cat', '.', 'Do', 'you', 'have', 'pets', 'too', '?', 'My', 'cat', 'likes', 'to', 'chase', 'mice', '.', 'My', 'dogs', 'like', 'to', 'chase', 'my', 'cat', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "print(sent_tokenize(\"I have two dogs and a cat. Do you have pets too? My cat likes to chase mice. My dogs like to chase my cat.\"))\n",
        "\n",
        "print(word_tokenize(\"I have two dogs and a cat. Do you have pets too? My cat likes to chase mice. My dogs like to chase my cat.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52HK8zV4ynct"
      },
      "source": [
        "We also might not want to have distinctions between words like \"This\" (that appears at the beginning of the phrase, therefore, it is capitalized) and \"this\". We can apply lower() on the whole text, but we might lose proper nouns this way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wE4rrCVyfo2"
      },
      "source": [
        "### Removing stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK16C8IryxNt"
      },
      "source": [
        "Stopwords are very common words that don't bring any information about the theme and meaning of the text (like pronouns, prepositions etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrpFwLnGy_v5",
        "outputId": "62d7686a-60e1-4c0e-d6fb-99ffc5183027"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMLWVA_ay6jr",
        "outputId": "07fd7961-821f-4467-9d9a-0aff64e92a32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "198\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\"]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(len(stopwords.words('english')))\n",
        "stopwords.words('english')[0:30]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWWRwSV2zuB5"
      },
      "source": [
        "### Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgxB_Iyqz5c5"
      },
      "source": [
        "We might want to find how many times we can find in a text the action of running. We might find the verb run in different forms, like: run, ran, running, runs etc. For this we can use the stemming process that results in the root of a word. There are multiple stemming algorithms. Three examples are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8qT4G3k0GpW",
        "outputId": "66370259-b80e-47e3-cde9-1bf940446952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run\n",
            "run\n",
            "ran\n",
            "are\n",
            "darl\n"
          ]
        }
      ],
      "source": [
        "# Porter stemmer\n",
        "ps = nltk.PorterStemmer()\n",
        "print(ps.stem(\"running\"))\n",
        "print(ps.stem(\"runs\"))\n",
        "print(ps.stem(\"ran\"))\n",
        "print(ps.stem(\"are\"))\n",
        "print(ps.stem(\"darling\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gIh0m8C0uYP",
        "outputId": "55e9bb12-9c1c-48cd-bc93-c03ade8a0e46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run\n",
            "run\n",
            "ran\n",
            "ar\n",
            "darl\n"
          ]
        }
      ],
      "source": [
        "# Lancaster stemmer - not recommended as it often results in overstemming\n",
        "ls = nltk.LancasterStemmer()\n",
        "print(ls.stem(\"running\"))\n",
        "print(ls.stem(\"runs\"))\n",
        "print(ls.stem(\"ran\"))\n",
        "print(ls.stem(\"are\"))\n",
        "print(ls.stem(\"darling\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQI6ylKc1XDD",
        "outputId": "bc51e8c0-32db-424a-d120-441adc96f6fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run\n",
            "run\n",
            "ran\n",
            "are\n",
            "darl\n"
          ]
        }
      ],
      "source": [
        "# Snowball stemmer (also known as Porter2)\n",
        "snb = nltk.SnowballStemmer(\"english\")\n",
        "print(snb.stem(\"running\"))\n",
        "print(snb.stem(\"runs\"))\n",
        "print(snb.stem(\"ran\"))\n",
        "print(snb.stem(\"are\"))\n",
        "print(snb.stem(\"darling\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uS41uO910g2"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyKYnQxq1549"
      },
      "source": [
        "The process of lematization returns the dictionary form of a word (canonical form). We will use WordNetLemmatizer, which requires WordNet, a lexical database of semantic relations between words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGDjkXkENNI3",
        "outputId": "75abad9b-1a15-4cf0-e618-9e84c4d3daec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQ0BAlDi1xDJ",
        "outputId": "b8f6583d-cf65-47f5-d1e9-bfaa3eff1615"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running\n",
            "run\n",
            "ran\n",
            "are\n",
            "darling\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lem=WordNetLemmatizer()\n",
        "print(lem.lemmatize(\"running\"))\n",
        "print(lem.lemmatize(\"runs\"))\n",
        "print(lem.lemmatize(\"ran\"))\n",
        "print(lem.lemmatize(\"are\"))\n",
        "print(lem.lemmatize(\"darling\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LohRJq_qMZFc"
      },
      "source": [
        "Note that lemmatization is slower than stemming. This is because lemmatization performs morphological analysis and derives the meaning of words from a dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofcTJID-2gaS"
      },
      "source": [
        "You can specify the part of speech for the given word and the results greatly improve:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKaeqHsX2kBT",
        "outputId": "12e7fa81-84fe-43c7-9c3c-7ef9f76ffb7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run\n",
            "run\n",
            "be\n"
          ]
        }
      ],
      "source": [
        "print(lem.lemmatize(\"running\", pos=\"v\"))\n",
        "print(lem.lemmatize(\"ran\", pos=\"v\"))\n",
        "print(lem.lemmatize(\"are\", pos=\"v\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0lIN-8z22WD"
      },
      "source": [
        "### Numeral conversion or removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_73IS7C3FMO"
      },
      "source": [
        "Sometimes we want to remove all numerals as they give no information about the category of the text, for example. Sometimes we need the numerals in order to programatically understand and save the information from the text in some form of knowledge representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1OMkQQh4wma",
        "outputId": "053a399d-0216-42da-82b8-0de059d38482"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: word2number\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=b76f60d654d0f8427131e0739b76e4bd63fb601d05fb91384f6a61a3bb977ae8\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/ef/ae/073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06\n",
            "Successfully built word2number\n",
            "Installing collected packages: word2number\n",
            "Successfully installed word2number-1.1\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting docopt>=0.6.2 (from num2words)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=c62c2b9e08a12ec9b5fc0af876d4260a555b1e0043750ea6fde7a484e89072f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.14\n"
          ]
        }
      ],
      "source": [
        "!pip install word2number\n",
        "!pip install num2words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt399Avo3EJT",
        "outputId": "9b53ed5f-c29d-429b-973c-8c693a6be03e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11\n",
            "23\n",
            "twelve\n",
            "one hundred and one\n",
            "two thousand and twenty-five\n"
          ]
        }
      ],
      "source": [
        "from word2number import w2n\n",
        "print(w2n.word_to_num(\"eleven\"))\n",
        "print(w2n.word_to_num(\"twenty three\"))\n",
        "\n",
        "from num2words import num2words\n",
        "print(num2words(12))\n",
        "print(num2words(101))\n",
        "print(num2words(2025))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdWkPFcj6pHm"
      },
      "source": [
        "### Bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-YRUTIaLP3w0"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I7tyf_bu6oWO"
      },
      "outputs": [],
      "source": [
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQukVmAh7Ms1",
        "outputId": "7600d41c-b634-4f8f-88f0-e389a0645ce0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25 bigrams found\n",
            "first 10 bigrams:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(('to', 'chase'), 14.555378652741947),\n",
              " (('and', 'a'), 8.699705569404067),\n",
              " (('do', 'you'), 8.699705569404067),\n",
              " (('pets', 'too'), 8.699705569404067),\n",
              " (('too', '?'), 8.699705569404067),\n",
              " (('cat', '.'), 6.994150686613864),\n",
              " (('my', 'cat'), 6.994150686613864),\n",
              " (('chase', 'mice'), 5.927116847164285),\n",
              " (('dogs', 'and'), 5.927116847164285),\n",
              " (('dogs', 'like'), 5.927116847164285)]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"I have two dogs and a cat. Do you have pets too? My cat likes to chase mice. My dogs like to chase my cat.\".lower()\n",
        "tokens =  word_tokenize(text)\n",
        "bigrams_finder = BigramCollocationFinder.from_words(tokens)\n",
        "bigram_measures = BigramAssocMeasures() # a collection of bigram association measures (i.e. scoring functions)\n",
        "\n",
        "# score_ngrams returns a sequence of (ngram, score) pairs ordered from highest to lowest score based on the scoring function provided\n",
        "bigram_scores = bigrams_finder.score_ngrams(bigram_measures.likelihood_ratio)\n",
        "print(len(bigram_scores), \"bigrams found\")\n",
        "\n",
        "# The likelihood ratio tells you whether two words appearing together is more likely due to a real connection between them\n",
        "# rather than just happening by chance.\n",
        "n_best = 10\n",
        "print(f\"first {n_best} bigrams:\")\n",
        "bigram_scores[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBDvNuaEQWEu",
        "outputId": "7692ca1f-325e-4e1b-d366-b2e21ef26263"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(('to', 'chase'), 14.555378652741947),\n",
              " (('and', 'a'), 8.699705569404067),\n",
              " (('do', 'you'), 8.699705569404067),\n",
              " (('pets', 'too'), 8.699705569404067),\n",
              " (('too', '?'), 8.699705569404067),\n",
              " (('cat', '.'), 6.994150686613864),\n",
              " (('my', 'cat'), 6.994150686613864),\n",
              " (('chase', 'mice'), 5.927116847164285),\n",
              " (('dogs', 'and'), 5.927116847164285),\n",
              " (('dogs', 'like'), 5.927116847164285),\n",
              " (('have', 'pets'), 5.927116847164285),\n",
              " (('have', 'two'), 5.927116847164285),\n",
              " (('i', 'have'), 5.927116847164285),\n",
              " (('like', 'to'), 5.927116847164285),\n",
              " (('likes', 'to'), 5.927116847164285),\n",
              " (('two', 'dogs'), 5.927116847164285),\n",
              " (('you', 'have'), 5.927116847164285),\n",
              " (('.', 'do'), 4.880620559635201),\n",
              " (('?', 'my'), 4.880620559635201),\n",
              " (('a', 'cat'), 4.880620559635201),\n",
              " (('cat', 'likes'), 4.880620559635201),\n",
              " (('mice', '.'), 4.880620559635201),\n",
              " (('chase', 'my'), 2.2590649092660344),\n",
              " (('my', 'dogs'), 2.2590649092660344),\n",
              " (('.', 'my'), 1.3695320221449936)]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigram_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xFxbhJwJVVW",
        "outputId": "131d5784-52f5-45d1-83a8-cf1141ca376d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('cat', '.'),\n",
              " ('my', 'cat'),\n",
              " ('to', 'chase'),\n",
              " ('.', 'do'),\n",
              " ('.', 'my'),\n",
              " ('?', 'my'),\n",
              " ('a', 'cat'),\n",
              " ('and', 'a'),\n",
              " ('cat', 'likes'),\n",
              " ('chase', 'mice')]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# to score bigrams by their frequency use raw_freq\n",
        "bigrams_finder.nbest(bigram_measures.raw_freq, n_best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwcqBQI1C9eA",
        "outputId": "2906250d-cd78-428c-a968-bdf0e4d35f31"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('cat', '.'), ('my', 'cat'), ('to', 'chase')]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can apply different filters. For example, ignore all bigrams which occur less than 2 times.\n",
        "bigrams_finder.apply_freq_filter(2)\n",
        "bigrams_finder.nbest(bigram_measures.raw_freq, n_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcFcUh363moa"
      },
      "source": [
        "### Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7un5ONZ23s0h"
      },
      "source": [
        "For these exercises you must use some raw text. Choose a (short) book from https://www.gutenberg.org. Each exercise is worth 0.1. The exercises finished during the laboratory are graded with 0.2.\n",
        "\n",
        "1. Download it through python (inside the code, so you don't have to upload the file too when you send the solution for this exercise) with urlopen() from module urllib and read the entire text in one single string. If the download takes too much time at each running, download the file, but leave the former instructions in a comment (to show that you know how to access an online file)\n",
        "\n",
        "2. Remove the header (keep only the text starting from the title).\n",
        "\n",
        "3. Print the number of sentences in the text. Print the average length (number of words) of a sentence.\n",
        "\n",
        "4. Find the collocations in the text (bigrams and trigrams) that appear at least 5 times. Print them only once, not each time they appear.\n",
        "\n",
        "5. Create a list of all the words (in lower case) from the text, without the punctuation.\n",
        "\n",
        "6. Print the first N most frequent words (alphanumeric strings) together with their number of appearances. You can use FreqDist from nltk.\n",
        "\n",
        "7. Remove stopwords and assign the result to variable lws.\n",
        "\n",
        "8. Apply stemming (Porter) on the list of words (lws). Print the first 200 words. Do you see any words that don't appear in the dictionary?\n",
        "\n",
        "9. Print a table of three columns (of size N, where N is the maximum length for the words in the text). The columns will be separated with the character \"|\". The head of the table will be: \"Porter    |Lancaster |Snowball\". The table will contain only the words that give different stemming results for the three stemmers (for example, suppose that we have both \"runs\" and \"being\" inside the text. The word \"runs\" should not appear in the list, as all three results are \"run\"; however \"being\" should appear in the table). The stemming result for the word for each stemmer will appear in the table according to the head of the table. The table will contain the results for the first NW words from the text (the number of rows will obviously be less than NW, as not all words match the requirements). For example, NW=500. Try to print only distinct results inside the table (for example, if a word has two occurences inside the text, and matches the requirments for appearing in the table, it should have only one corresponding row).\n",
        "\n",
        "10. Print a table of two columns, simillar to the one above, that will compare the results of stemming and lemmatization. The head of the table will contain the values: \"Snowball\" and \"WordNetLemmatizer\". The table must contain only words that give different results in the process of stemming and lemmatization (for example, the word \"running\"). The table will contain the results for the first NW words from the text (the number of rows will obviously be less than NW, as not all words match the requirements). For example, NW=500. Try to print only distinct results inside the table (for example, if a word has two occurnces inside the text, and matches the requirments for appearing in the table, it should have only one corresponding row).\n",
        "\n",
        "11. Print the first N most frequent lemmas (after the removal of stopwords) together with their number of appearances.\n",
        "\n",
        "12. Change all the numbers from lws into words. Print the number of changes, and also the portion of list that contains first N changes (for example N=10).\n",
        "\n",
        "13. Create a function that receives an integer N and a word W as parameter (it can also receive the list of words from the text). We want to print the concordance data for that word. This means printing the window of text (words on consecutive positions) of length N, that has the givend word W in the middle. For example, for the text \"I have two dogs and a cat. Do you have pets too? My cat likes to chase mice. My dogs like to chase my cat.\" and a window of length 3, the concordance data for the word \"cat\" would be [\"dogs\", \"cat\", \"pets\"] and [\"pets\",\"cat\", \"likes\"] (we consider the text without stopwords and punctuation).\n",
        "\n",
        "14. In the previous exercise, the window of text may contain words from different sentences. Create a second function that prints windows of texts that contain words only from the phrase containing word W. We want to print concordance data for all the inflexions of word W."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "argiGEXvOF5N"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "url = \"https://www.gutenberg.org/cache/epub/75518/pg75518.txt\"  \n",
        "with urlopen(url) as response:\n",
        "    text_content = response.read().decode(\"utf-8\")  \n",
        "print(text_content) \n",
        "import re\n",
        "text_content = re.sub(r'\\\\r\\\\n', ' ', text_content)\n",
        "text_content = re.sub(r'\\\\xe2\\\\x80\\\\x99', '\\'', text_content) # \\xe2\\x80\\x99 -> '\n",
        "text_content = re.sub(r'\\\\xe2\\\\x80\\\\x9c|\\\\xe2\\\\x80\\\\x9d', '\\\"', text_content)\n",
        "text_content = re.sub(r'\\\\xe2\\\\x80\\\\x94', ' — ', text_content)\n",
        "text_content = re.sub(r'\\\\xc3\\\\xa7',  'c', text_content) # ç -> c\n",
        "text_content = text_content.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "title_marker = \"the seeking\"\n",
        "start_index = text_content.find(title_marker)\n",
        "if start_index != -1:\n",
        "    text_content = text_content[start_index:]  \n",
        "print(text_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentences = sent_tokenize(text_content)\n",
        "tokens = word_tokenize(text_content)\n",
        "print(len(sentences))\n",
        "print(len(tokens) / len(sentences))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens = word_tokenize(text_content)\n",
        "\n",
        "bigrams = nltk.bigrams(tokens)\n",
        "trigrams = nltk.trigrams(tokens)\n",
        "\n",
        "bgram_dist = nltk.FreqDist(bigrams)\n",
        "trigram_dist = nltk.FreqDist(trigrams)\n",
        "for k,v in bgram_dist.items():\n",
        "    if(v == 5):\n",
        "        print(k, v)\n",
        "\n",
        "for l,m in  trigram_dist.items():\n",
        "    if(m == 5):\n",
        "        print(l, m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "words = re.findall(r\"\\b[a-zA-Z0-9]+\\b\", text_content.lower()) \n",
        "print(words[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "most_freq = nltk.FreqDist(words)\n",
        "stop = 3\n",
        "counter = 0\n",
        "for i,j in most_freq.items():\n",
        "  print(i,j)\n",
        "  counter += 1\n",
        "  if counter > stop:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "words = [word for word in words if word not in stop_words]\n",
        "most_freq = nltk.FreqDist(words)\n",
        "stop = 3\n",
        "counter = 0\n",
        "for i,j in most_freq.items():\n",
        "  print(i,j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "snb = nltk.SnowballStemmer(\"english\")\n",
        "stemm = [snb.stem(word) for word in words]\n",
        "print(stemm[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "\n",
        "distinct_words = {}\n",
        "\n",
        "for word in set(words): \n",
        "    porter_stem = porter.stem(word)\n",
        "    lancaster_stem = lancaster.stem(word)\n",
        "    snowball_stem = snowball.stem(word)\n",
        "    \n",
        "    if porter_stem != lancaster_stem or porter_stem != snowball_stem or lancaster_stem != snowball_stem:\n",
        "        distinct_words[word] = (porter_stem, lancaster_stem, snowball_stem)\n",
        "\n",
        "max_word_length = max(len(word) for word in distinct_words.keys())\n",
        "\n",
        "\n",
        "print(f\"{'Porter':<{max_word_length}} | {'Lancaster':<{max_word_length}} | {'Snowball':<{max_word_length}}\")\n",
        "\n",
        "for word, (porter_stem, lancaster_stem, snowball_stem) in distinct_words.items():\n",
        "    print(f\"{porter_stem:<{max_word_length}} | {lancaster_stem:<{max_word_length}} | {snowball_stem:<{max_word_length}}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "snowball_stemmer = SnowballStemmer('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Create sets to store words where stemming and lemmatization differ\n",
        "stemmed_words = set()\n",
        "lemmatized_words = set()\n",
        "comparison_table = []\n",
        "\n",
        "# Loop through the words and compare the results of stemming and lemmatization\n",
        "for word in words:\n",
        "    # Apply stemming\n",
        "    stemmed = snowball_stemmer.stem(word)\n",
        "    \n",
        "    # Apply lemmatization\n",
        "    lemmatized = lemmatizer.lemmatize(word)\n",
        "    \n",
        "    # Check if stemming and lemmatization give different results\n",
        "    if stemmed != lemmatized:\n",
        "        if stemmed not in stemmed_words and lemmatized not in lemmatized_words:\n",
        "            comparison_table.append([stemmed, lemmatized])\n",
        "            stemmed_words.add(stemmed)\n",
        "            lemmatized_words.add(lemmatized)\n",
        "\n",
        "if (max(len(word) for word in stemmed_words) > max(len(word) for word in lemmatized_words)):\n",
        "    max_word_length = max(len(word) for word in stemmed_words)\n",
        "else:\n",
        "    max_word_length = max(len(word) for word in lemmatized_words)\n",
        "\n",
        "print(f\"{'Stemmed':<{max_word_length}} | {'Lemmatized':<{max_word_length}}\")\n",
        "for stem, lem in comparison_table:\n",
        "    print(f\"{stem:<{max_word_length}} | {lem:<{max_word_length}}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "N = 10\n",
        "words = [word for word in words if word.lower() not in stop_words]\n",
        "lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
        "freqWords = nltk.FreqDist(lemmas).most_common()\n",
        "print(freqWords[:N])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from word2number import w2n\n",
        "numbers = []\n",
        "for word in words:\n",
        "  try:\n",
        "    numbers.append(w2n.word_to_num(word))\n",
        "  except Exception:\n",
        "    pass\n",
        "print(numbers[:N])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exercises 13 and 14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def function(N: int, W: str, text):\n",
        "  '''\n",
        "    Returns the window of words but doesnt take into consideration stop_words as it is not mentioned.\n",
        "  '''\n",
        "  \n",
        "  if W not in words:\n",
        "    print(f\"'{W}' not found in the text\")\n",
        "    return\n",
        "  stemmer = PorterStemmer()\n",
        "  W = stemmer.stem(W.lower()) \n",
        "  lists = []\n",
        "  indexes = [index for (index, word) in enumerate(text) if word == W]\n",
        "  c = 0\n",
        "  for target_index in indexes:\n",
        "    start = max(target_index - N // 2, 0)\n",
        "    finish = min(target_index + N // 2 + 1, len(words))\n",
        "    lists.append(text[start:finish])\n",
        "    print(f\"Concordance {c + 1}: {\" \".join(text[start:finish])}\")\n",
        "    c +=1 \n",
        "  return lists\n",
        "\n",
        "print(function(3, \"word\", \"Trying to find word in words, but the word repeats\".split()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
