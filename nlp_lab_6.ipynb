{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCWalIwy0FCU"
      },
      "source": [
        "## Supervised WSD with Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRCkllW-0S1A"
      },
      "source": [
        "We will use the Bayes classifier to label (classify) the words with a WordNet sense. For this we need a context window surrounding the target word (the word for which we search the sense). The context window should contain only \"content words\" (words with important meaning that bring information, like nouns, verbs etc),\n",
        "\n",
        "We note P(s|c) the probability for sense s in the context c. For each such sense of the target word the probability is computed and we take the sense with the highest probability compared to the others.\n",
        "\n",
        "In order to compute the probability P(s|c), we use the formula: P(s|c)=(P(c|s)*P(s))/P(c). P(s) is the probability of a sense without any context. For P(c|s) we would need a training set (with texts that contain the target word, already labeled with its correct sense).\n",
        "\n",
        "NLTK already has the classifier implemented. We can use the NLTK NaiveBayesClassifier: https://www.nltk.org/_modules/nltk/classify/naivebayes.html\n",
        "\n",
        "The Naive Bayes classifier will first compute the prior probability for the senses (or, generally speaking, for the class labels) - this is determined by the label's frequncy in the training set. The features are used to see the likelyhood of having that label in a given context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jky4Ndf32CK"
      },
      "source": [
        "nltk.NaiveBayesClassifier.train(train_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D0qiMwl3-Te"
      },
      "source": [
        "**train_set** must contain a list with the classes and features for each class. The **train_set** list will contain tuples of two elements. The first element is a dictionary with the features (name and value of each feature), the second element is the class label.\n",
        "\n",
        "You can also use Naive Bayes classifier from sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
        "\n",
        "Useful link: https://www.nltk.org/book/ch06.html\n",
        "\n",
        "For today's task, you need to train the NLTK Bayes classifier on senseval, on a word of your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j_5cB0e5Jt-",
        "outputId": "4c9cb927-bd56-42a6-8d19-c5e0868b6d7a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]   Package senseval is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('senseval')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVMfTgX3ODKz",
        "outputId": "feeae625-daa3-463c-bc34-100a2d8b72db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hard.pos', 'interest.pos', 'line.pos', 'serve.pos']"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "senseval.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz0wu2XG40hU",
        "outputId": "2b1cbfb7-5da8-4ee9-c473-700cbd0694dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SensevalInstance(word='hard-a', position=3, context=[('i', 'PRP'), ('find', 'VBP'), ('it', 'PRP'), ('hard', 'JJ'), ('to', 'TO'), ('believe', 'VB'), ('that', 'IN'), ('the', 'DT'), ('sacramento', 'NNP'), ('river', 'NNP'), ('will', 'MD'), ('ever', 'RB'), ('be', 'VB'), ('quite', 'RB'), ('the', 'DT'), ('same', 'JJ'), (',', ','), ('although', 'IN'), ('i', 'PRP'), ('certainly', 'RB'), ('wish', 'VBP'), ('that', 'IN'), ('i', 'PRP'), (\"'m\", 'VBP'), ('wrong', 'JJ'), ('.', '.')], senses=('HARD1',))\n",
            "('HARD1',)\n",
            "SensevalInstance(word='hard-a', position=6, context=[('he', 'PRP'), ('said', 'VBD'), ('tuesday', 'NNP'), ('there', 'EX'), ('are', 'VBP'), ('no', 'DT'), ('hard', 'JJ'), ('feelings', 'NNS'), ('.', '.')], senses=('HARD2',))\n",
            "('HARD2',)\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import senseval\n",
        "instances = senseval.instances('hard.pos')\n",
        "print(instances[2])\n",
        "print(instances[2].senses)\n",
        "print(instances[3500])\n",
        "print(instances[3500].senses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjSz5kwI5l5O"
      },
      "source": [
        "The classes used for training are the senses, and the features are the surrounding words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QKmBzUl51iK"
      },
      "source": [
        "### Exercise (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BIrsLYK56Sw"
      },
      "source": [
        "Apply Naive Bayes classifier on one of the files in senseval dataset (for example, for the word interest). Use 90% of the phrases for training and 10% for testing the classifier. The phrases should be taken in a random order (shuffle the phrases before training and testing). For the testing set print the predictions of the classifier and the correct labels from the corpus and also print \"true\" if they are the same, and \"false\" if they are different. In the end, print the accuracy of the classifier. Write the output to a txt file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import senseval\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "instances = senseval.instances('hard.pos')\n",
        "contexts = [\" \".join(inst.context[0]) for inst in instances]\n",
        "labels   = [inst.senses[0] for inst in instances]\n",
        "random.seed(13)\n",
        "idx = list(range(len(contexts)))\n",
        "random.shuffle(idx)\n",
        "split = int(0.9 * len(idx))\n",
        "train_idx, test_idx = idx[:split], idx[split:]\n",
        "\n",
        "train_texts = [contexts[i] for i in train_idx]\n",
        "train_lbls  = [labels[i]   for i in train_idx]\n",
        "test_texts  = [contexts[i] for i in test_idx]\n",
        "test_lbls   = [labels[i]   for i in test_idx]\n",
        "\n",
        "vect = CountVectorizer()\n",
        "X_train = vect.fit_transform(train_texts)\n",
        "X_test  = vect.transform(test_texts)\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, train_lbls)\n",
        "preds = clf.predict(X_test)\n",
        "\n",
        "out_file = 'naive_bayes_interest_output.txt'\n",
        "with open(out_file, 'w') as f:\n",
        "    for p, t in zip(preds, test_lbls):\n",
        "        f.write(f\"{p}\\t{t}\\t{str(p==t).lower()}\\n\")\n",
        "    acc = sum(p==t for p,t in zip(preds, test_lbls)) / len(test_lbls)\n",
        "    f.write(f\"Accuracy: {acc:.4f}\\n\")\n",
        "\n",
        "print(f\"Wrote results to ./{out_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QloH4IOi8x7J"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxrwTz7L9hi_"
      },
      "source": [
        "BERT (Bidirectional Encoder Representations from Transformers) was introduced in 2018. BERT, as opposed to word2vec (context-free model), is a contextual model, trained using the Masked Language Modeling (MLM) technique. This means that part of the words in the input are masked (considered unknown) and BERT is trained to predict them based on their context in the input text. It is also trained to predict what sentence comes before a given sentence (Next Sentence Prediction). As a result of this training process, BERT learns contextual representations of tokens based on their context. BERT is **bidirectional** as it considers both the left and right context of each word in the input during training.\n",
        "\n",
        "**Transformers** are a deep learning architecture that use the attention mechanism (self-attention) to create vector encodings of the words in a text, based on their surrounding text (context). BERT has encoder-only transformer architecture. The encoder receives the input and turns into an embedding to be used by the neural network.\n",
        "\n",
        "**The attention mechanism** assigns weights for each token in the input in relation to the other tokens in the context of the input, thus obtaining a contextual model. Attention helps decide how important a token is in an input, and how it depends on the other values in the input. Bert uses a multihead attention mechanism, which means it looks at multiple words at the same time.\n",
        "\n",
        "We will use BERT from [Hugging Face](https://huggingface.co/google-bert/bert-base-uncased)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IstOnQZA1TU"
      },
      "source": [
        "### Bert tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA6JVibfBW1e"
      },
      "source": [
        "It is in charge of preparing the inputs for the model. The BERT tokenizer uses subword-based tokenization, which splits unknown words into smaller words or characters such that the model can derive some meaning from the tokens. BERT uses the [WordPiece](https://paperswithcode.com/method/wordpiece) algorithm to generate the vocabulary. The WordPiece algorithm generates subwords based on the likelihood of characters occurring together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNnbDvSRCtKV"
      },
      "source": [
        "### Training and fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OeUtUD9HYPM"
      },
      "source": [
        "BERT is usually pretrained, and if we want to specialize it on a task we fine tune it by training it on our data for a small amount of epochs (usually 2-4), with a learning rate from the following: 3e-4, 1e-4, 5e-5, 3e-5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsI_h2piHttY"
      },
      "source": [
        "### Important terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlzMpQBPHw-J"
      },
      "source": [
        "- **Logits** - the predicted values (the value of the last layer)\n",
        "- **Loss** - the difference between predicted and true values\n",
        "- **batch** - input data is divided into batches that are delivered to the network; recommended batch sizes: 8, 16, 32, 64, 128\n",
        "- **optimizer** - adjusts the parameters of the model in order to minimize the loss function\n",
        "- **gradients** - represent the partial derivatives of the loss function with respect to the model's parameters (weights and biases). Gradients are used to update the parameters in a way that minimizes the loss\n",
        "\n",
        "References:\n",
        "- https://www.projectpro.io/article/bert-nlp-model-explained/558\n",
        "- https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
        "- https://huggingface.co/docs/transformers/main_classes/tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EulV3UXJIVTJ"
      },
      "source": [
        "### Exercises (2 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUT4phzKX5vz"
      },
      "source": [
        "Note: Exercises 10 - 14 are closely related. Read all of them before starting exercise 10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOBHTKE_IgVe"
      },
      "source": [
        "1. Install the transformers module.\n",
        "2. Open the corpus file from senseval for a word of your choice. Print the number of phrases (instances).\n",
        "3. Print the first 10 phrases as raw text (untagged).\n",
        "4. Create a DataFrame based on the sentences in the text with the columns: text, label, id. Each row will correspond to an entry in the corpus. The column text would contain the phrase from the entry (as raw text - untagged), the label will be the sense for the target word (the label should be an int), the id is the number of the phrase.\n",
        "5. Shuffle the data.\n",
        "6. For the training set, use 90% of instances to train a BERT classifier. Try to find the sense of the word on the rest of 10% of instances (test set). The classes used for training are the senses.\n",
        "7. Create a pretrained classification model with BertForSequenceClassification. The number of labels is actually the number of classes. Set the output attentions an hidden states on false\n",
        "8. Use BertTokenizer from pretrained model \"bert-base-uncased\", in lowercase.\n",
        "9. Use batch_encode_plus method of the tokenizer to encode the texts. Use the longest padding and return the attention mask.\n",
        "10. Use 2 epochs to train the data using AdamW optimizer with learning rate 5e-5 and with batch sizes of:\n",
        "    - 16\n",
        "    - 32\n",
        "    - 64\n",
        "11. For each epoch load the batches and train the model with the batches (this is the step where we do the fine tuning). Don't forget to use zero_grad() on the optimizer.\n",
        "12. For each batch size, try training with 2, 3 and 4 epochs. Print the loss both for training and testing. Choose the best number of epochs for each batch size based on the values of the loss functions.\n",
        "13. Print how much time each training epoch took.\n",
        "14. Justify the choices at ex. 12 with 3 tables (columns will correspond to the number of epochs and rows to the loss function values for training and testing).\n",
        "15. Classify the test set using the best model you trained (i.e. best loss function). You again load in batches the data and check the predictions.\n",
        "16. Print the test accuracy, precision, recall, and weighted f1-score of the best model. You can use sklearn.\n",
        "17. Plot the confusion matrix. You can again use sklearn.\n",
        "18. Change the code so that the computations are made on GPU (for NVIDIA, method cuda())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Each Cell represents one exercise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install transformers\n",
        "import time\n",
        "import torch_optimizer as optim\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "# import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('senseval')\n",
        "from nltk.corpus import senseval\n",
        "\n",
        "word = 'interest'\n",
        "instances = senseval.instances(f\"{word}.pos\")\n",
        "print(f\"Total instances for '{word}': {len(instances)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, inst in enumerate(instances[:10]):\n",
        "    context = ' '.join(inst.context[0])\n",
        "    senses = inst.senses\n",
        "    print(f\"{i+1}. Context: {context}\\n   Senses: {senses}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = []\n",
        "for i, inst in enumerate(instances):\n",
        "    text = ' '.join(inst.context[0])\n",
        "    label = inst.senses[0]\n",
        "    data.append({'text': text, 'label': label, 'id': i})\n",
        "df = pd.DataFrame(data)\n",
        "df['label'], label_names = pd.factorize(df['label'])\n",
        "print(f\"Label mapping: {dict(enumerate(label_names))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.sample(frac=1, random_state=13).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(\n",
        "    df, test_size=0.1, stratify=df['label'], random_state=13\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_labels = len(label_names)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    'bert-base-uncased', do_lower_case=True\n",
        ")\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased', num_labels=num_labels,\n",
        "    output_attentions=False, output_hidden_states=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_texts(texts):\n",
        "    return tokenizer.batch_encode_plus(\n",
        "        texts, add_special_tokens=True, max_length=128,\n",
        "        padding='longest', truncation=True,\n",
        "        return_attention_mask=True, return_tensors='pt'\n",
        "    )\n",
        "\n",
        "train_enc = encode_texts(train_df['text'].tolist())\n",
        "test_enc = encode_texts(test_df['text'].tolist())\n",
        "\n",
        "class WSDDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_sizes = [16, 32, 64]\n",
        "results = {}\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "    train_ds = WSDDataset(train_enc, train_df['label'].tolist())\n",
        "    test_ds  = WSDDataset(test_enc,  test_df['label'].tolist())\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    test_loader  = DataLoader(test_ds,  batch_size=batch_size)\n",
        "    for epochs in [2, 3, 4]:\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "        train_losses, test_losses, times = [], [], []\n",
        "        for epoch in range(epochs):\n",
        "            t0 = time.time()\n",
        "            model.train()\n",
        "            total_train_loss = 0\n",
        "            for batch in train_loader:\n",
        "                b_ids = batch['input_ids'].to(device)\n",
        "                b_mask = batch['attention_mask'].to(device)\n",
        "                b_labels = batch['labels'].to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(\n",
        "                    input_ids=b_ids,\n",
        "                    attention_mask=b_mask,\n",
        "                    labels=b_labels\n",
        "                )\n",
        "                loss = outputs.loss\n",
        "                total_train_loss += loss.item()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            avg_train = total_train_loss / len(train_loader)\n",
        "            train_losses.append(avg_train)\n",
        "            model.eval()\n",
        "            total_eval_loss = 0\n",
        "            for batch in test_loader:\n",
        "                b_ids = batch['input_ids'].to(device)\n",
        "                b_mask = batch['attention_mask'].to(device)\n",
        "                b_labels = batch['labels'].to(device)\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(\n",
        "                        input_ids=b_ids,\n",
        "                        attention_mask=b_mask,\n",
        "                        labels=b_labels\n",
        "                    )\n",
        "                total_eval_loss += outputs.loss.item()\n",
        "            avg_test = total_eval_loss / len(test_loader)\n",
        "            test_losses.append(avg_test)\n",
        "\n",
        "            epoch_time = time.time() - t0\n",
        "            times.append(epoch_time)\n",
        "            print(f\"Batch {batch_size} | Epoch {epoch+1}/{epochs} | Train Loss: {avg_train:.4f} | Test Loss: {avg_test:.4f} | Time: {epoch_time:.1f}s\")\n",
        "        results[(batch_size, epochs)] = {\n",
        "            'train_losses': train_losses,\n",
        "            'test_losses': test_losses,\n",
        "            'times': times\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for batch_size in batch_sizes:\n",
        "    df_summary = pd.DataFrame({\n",
        "        'Epochs': [2, 3, 4],\n",
        "        'Train Loss': [results[(batch_size,e)]['train_losses'][-1] for e in [2,3,4]],\n",
        "        'Test Loss':  [results[(batch_size,e)]['test_losses'][-1]  for e in [2,3,4]]\n",
        "    })\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    print(df_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# This final part represent exercises 15-18 as splittin them appart would've been weird"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_bs = 16\n",
        "best_ep = 2\n",
        "model.eval()\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    WSDDataset(test_enc, test_df['label'].tolist()),\n",
        "    batch_size=best_bs\n",
        ")\n",
        "all_preds, all_labels = [], []\n",
        "for batch in test_loader:\n",
        "    b_ids = batch['input_ids'].to(device)\n",
        "    b_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].numpy()\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids=b_ids, attention_mask=b_mask).logits\n",
        "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "    all_preds.extend(preds)\n",
        "    all_labels.extend(labels)\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    all_labels, all_preds, average='weighted'\n",
        ")\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(f\"\\nTest Accuracy:  {acc:.4f}\")\n",
        "print(f\"Precision:      {precision:.4f}\")\n",
        "print(f\"Recall:         {recall:.4f}\")\n",
        "print(f\"Weighted F1:    {f1:.4f}\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "ticks = range(len(label_names))\n",
        "plt.xticks(ticks, label_names, rotation=45)\n",
        "plt.yticks(ticks, label_names)\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
