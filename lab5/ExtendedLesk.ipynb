{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFRnaDiVWJF3"
      },
      "source": [
        "### Extended Gloss Overlaps (Extended Lesk)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlKhXV45WX-b"
      },
      "source": [
        "The algorithm measures the relatedness of two words. Just like Simple Lesk, it counts the overlaps of glosses, however it takes into account the related glosses of the two words as well.\n",
        "\n",
        "Suppose we want to obtain the sense for a word in a certain context (for example a sentence or just a window of text). The steps of the algorithm are:\n",
        "\n",
        "1. We first tag the words in the sentence with their part of speech\n",
        "2. For each word we obtain the list of synsets corresponding to that part of speech.\n",
        "3. For each synset we obtain the glosses of the synsets for all:\n",
        "- hypernyms\n",
        "- hyponyms\n",
        "- meronyms\n",
        "- holonyms\n",
        "- troponyms\n",
        "- attributes\n",
        "- similar-tos\n",
        "- also-sees\n",
        "\n",
        "It is good to use a structure that shows for each gloss from which synset it comes from (in order to do the tests in the exercise). We add them all in a list with all the glosses (for each target word). We call these lists \"extended glosses\".\n",
        "\n",
        "4. For each synset of the target word (i.e. the word for which we want to obtain the sense) we compute a score by counting the overlaps in the synset with all the other synsets corresponding to the words in the context. In computing the score, for each single word that appears in both extended glosses we add 1. However if it appears in a common phrase, supposing the length of common phrase is L, we add L^2 (for example, if \"white bread\" appears in both glosses, we add 4). We obviusly don't add the score for the separate words in the phrase. We try to find the longest common sequences of consecutive words (it shouldn't start or end with a pronoun, preposition, article or conjunction in both glosses). In order to avoid counting the same overlap multiple times for the same two glosses, after counting the overlap you should replace the sequence of words with a special string that doesn't appear in the text (don't remove it completely as you may obtain false overlaps). For example, you can use as special string \"###\".\n",
        "5. After computing the score for each synset of the target word, choose as result the synset with the highest score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dGbffOeaQhm"
      },
      "source": [
        "### Extended Lesk Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sJFki6KaM7S"
      },
      "source": [
        "Implement extended Lesk algorithm. Experiment with the measure by using different WordNet relations in computing the score. For a list of 7-10 synsets, print the measure for each pair of synsets (without repeating the synsets) with five different sets of at least three relations taken into acount in measuring the score. Write the observations. Just like in the exercise from the previous lab, try to obtain the word sense for the given text and word and print its definition. Can you find a text and word where simple Lesk gives the wrong answer, however extended Lesk gives the right answer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqWxeR3vWBDY"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.wsd import lesk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "stopwords_ = stopwords.words('english')\n",
        "\n",
        "synset_names = ['dog.n.01', 'cat.n.01', 'car.n.01', 'bicycle.n.01', 'fruit.n.01', 'banana.n.02', 'computer.n.01']\n",
        "synsets = [wn.synset(name) for name in synset_names]\n",
        "\n",
        "relation_sets = {\n",
        "    'set1': ['hypernyms', 'hyponyms', 'member_meronyms'],\n",
        "    'set2': ['hypernyms', 'attributes', 'similar_tos'],\n",
        "    'set3': ['hypernyms', 'part_holonyms', 'member_holonyms'],\n",
        "    'set4': ['hyponyms', 'substance_meronyms', 'verb_troponyms'],\n",
        "    'set5': ['also_sees', 'attributes', 'member_meronyms']\n",
        "}\n",
        "\n",
        "def tokenize(text):\n",
        "    return [t.lower() for t in nltk.tokenize.word_tokenize(text) if t.isalpha()]\n",
        "\n",
        "\n",
        "def get_extended_gloss(syn, relations):\n",
        "  glosses = [syn.definition()]\n",
        "  glosses += syn.examples()\n",
        "  for rel in relations:\n",
        "    if hasattr(syn, rel):\n",
        "      related = getattr(syn, rel)()\n",
        "      for r in related:\n",
        "        glosses.append(r.definition())\n",
        "        glosses += r.examples()\n",
        "  return glosses\n",
        "\n",
        "g1 = get_extended_gloss(synsets[2], relation_sets['set1'])\n",
        "g2 = get_extended_gloss(synsets[3], relation_sets['set1'])\n",
        "\n",
        "def longest_common_sequence(a, b):\n",
        "  dp = [[0] * (len(b) + 1) for _ in range(len(a) + 1)]\n",
        "  max_len, end_i, end_j = 0, 0, 0\n",
        "  for i in range(1, len(a) + 1):\n",
        "    for j in range(1, len(b) + 1):\n",
        "      if a[i - 1] == b[j - 1]:\n",
        "        dp[i][j] = dp[i - 1][j - 1] + 1\n",
        "        if dp[i][j] > max_len:\n",
        "          max_len = dp[i][j]\n",
        "          end_i, end_j = i, j\n",
        "  return max_len, end_i, end_j\n",
        "\n",
        "def overlap_score(gloss1, gloss2):\n",
        "  g1 = tokenize(gloss1)\n",
        "  g2 = tokenize(gloss2)\n",
        "  score = 0\n",
        "  while True:\n",
        "    L, i, j = longest_common_sequence(g1, g2)\n",
        "    if L <= 1:\n",
        "      break\n",
        "    phrase = g1[i - L:i]\n",
        "    if phrase[0] not in stopwords_ and phrase[-1] not in stopwords_:\n",
        "      score += L * L\n",
        "    g1 = g1[:i - L] + [\"###\"] + g1[i:]\n",
        "    g2 = g2[:j - L] + [\"###\"] + g2[j:]\n",
        "  for w in set(g1) & set(g2):\n",
        "    if w != \"###\" and w not in stopwords_:\n",
        "      score += 1\n",
        "  return score\n",
        "\n",
        "def extended_overlap(s1, s2, relations):\n",
        "  glosses1 = get_extended_gloss(s1, relations)\n",
        "  glosses2 = get_extended_gloss(s2, relations)\n",
        "  total = 0\n",
        "  for g1 in glosses1:\n",
        "    for g2 in glosses2:\n",
        "      total += overlap_score(g1, g2)\n",
        "  return total\n",
        "\n",
        "for i in range(len(synsets)):\n",
        "  for j in range(i+1, len(synsets)):\n",
        "    s1, s2 = synsets[i], synsets[j]\n",
        "    scores = {name: extended_overlap(s1, s2, rels) for name, rels in relation_sets.items()}\n",
        "    print(f\"{s1.name()} & {s2.name()}: {scores}\")\n",
        "\n",
        "context = \"He went fishing for bass in the stream.\"\n",
        "simple = lesk(nltk.tokenize.word_tokenize(context), 'bass', 'n')\n",
        "print(\"\\nSimple Lesk: \", simple, \": \\n\", simple.definition() if simple else None)\n",
        "\n",
        "best_score = -1\n",
        "best_syn = None\n",
        "for s in wn.synsets('bass', 'n'):\n",
        "  score = sum(extended_overlap(s, s2, relation_sets['set1'])\n",
        "              for w in ['fishing', 'stream']\n",
        "              for s2 in wn.synsets(w, 'n'))\n",
        "  if score > best_score:\n",
        "    best_score = score\n",
        "    best_syn = s\n",
        "print(\"\\nExtended Lesk:\", best_syn, \": \\n\", best_syn.definition())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
